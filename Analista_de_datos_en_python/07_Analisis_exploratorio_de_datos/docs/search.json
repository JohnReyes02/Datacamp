[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis Exploratorio de Datos en Python",
    "section": "",
    "text": "Bienvenida\nEste curso interactivo cubre el proceso de exploración y análisis de datos en Python, desde entender un nuevo dataset hasta la limpieza e imputación de valores.\n📊 Nivel: Intermedio\n🕒 Duración estimada: 4 horas\n🎥 Incluye código, visualizaciones y ejercicios",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#módulos-del-curso",
    "href": "index.html#módulos-del-curso",
    "title": "Análisis Exploratorio de Datos en Python",
    "section": "Módulos del curso",
    "text": "Módulos del curso\n\nConocer un conjunto de datos\n\nLimpieza e imputación de datos\nRelaciones en los datos\nConvertir el análisis exploratorio en acción",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Análisis Exploratorio de Datos en Python",
    "section": "Datasets",
    "text": "Datasets\nEste curso utiliza los siguientes archivos:\n\nunemployment.csv\ndata_science_salaries.csv\nbooks.csv\ndivorce.csv\nplanes.csv",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html",
    "href": "01_Conocer_un_conjunto_de_datos.html",
    "title": "Conocer un conjunto de datos",
    "section": "",
    "text": "Exploración inicial\n¿Cuál el la mejor manera de abordar un nuevo conjunto de datos? Aprende a validar y resumir datos categóricos y numéricos y a crear visualizaciones Seaborn para comunicar tus conclusiones.\nbooks = pd.read_csv('books.csv')\nbooks.head()\nbooks.info()\nbooks.value_counts('genre')\nbooks.describe()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.histplot(data=books, x='rating')\nplt.show()\nsns.histplot(data=books, x='rating', binwidth=.1)\nplt.show()",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#exploración-inicial",
    "href": "01_Conocer_un_conjunto_de_datos.html#exploración-inicial",
    "title": "Conocer un conjunto de datos",
    "section": "",
    "text": "Análisis Exploratorio de Datos\n\nEs el proceso de limpiar y revisar datos para:\n\nObterner información (Estadística descriptiva, correlaciones)\nGenrar hipótesis\n\n\nUna primera mirada con .head()\n\n\n\n\nReuniendo más .info()\n\n\n\n\nUna mirada cercana a las columnas categóricas\n\n\n\n\nColumnas numéricas con .describe()\n\n\n\n\nVisualizando datos numéricos\n\n\n\n\nAjustando la anchura del bin\n\n\n\n\nFunciones para la exploración inicial\nEstás investigando las tasas de desempleo en todo el mundo y te han dado un nuevo conjunto de datos con el que trabajar. Los datos se han guardado y cargado para ti como un DataFrame de pandas llamado unemployment. Nunca antes habías visto los datos, así que tu primera tarea es utilizar unas cuantas funciones de pandas para conocer estos nuevos datos.\n\nimport pandas as pd\n\nruta = './data/clean_unemployment.csv'\nunemployment = pd.read_csv(ruta)\n\n\nInstrucciones\n\nUtiliza una función de pandas para imprimir las cinco primeras filas del DataFrame unemployment.\n\n\n# Print the first five rows of unemployment\nprint(unemployment.head())\n\n  country_code          country_name      continent   2010   2011   2012  \\\n0          AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1          AGO                Angola         Africa   9.43   7.36   7.35   \n2          ALB               Albania         Europe  14.09  13.48  13.38   \n3          ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4          ARG             Argentina  South America   7.71   7.18   7.22   \n\n    2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0  11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1   7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2  15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3   2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4   7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n\n\n\nUtiliza una función pandas para imprimir un resumen de los valores y tipos de datos de las columnas que no faltan del DataFrame unemployment.\n\n\n# Print a summary of non-missing values and data types in the unemployment DataFrame]\nprint(unemployment.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 182 entries, 0 to 181\nData columns (total 15 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   country_code  182 non-null    object \n 1   country_name  182 non-null    object \n 2   continent     177 non-null    object \n 3   2010          182 non-null    float64\n 4   2011          182 non-null    float64\n 5   2012          182 non-null    float64\n 6   2013          182 non-null    float64\n 7   2014          182 non-null    float64\n 8   2015          182 non-null    float64\n 9   2016          182 non-null    float64\n 10  2017          182 non-null    float64\n 11  2018          182 non-null    float64\n 12  2019          182 non-null    float64\n 13  2020          182 non-null    float64\n 14  2021          182 non-null    float64\ndtypes: float64(12), object(3)\nmemory usage: 21.5+ KB\nNone\n\n\n\nImprime las estadísticas de resument (recuento, media, desviación estándar, valores mínimo, máximo y cuartil) de cada columna numérica en unemployment.\n\n\n# Print summary statistics for numerical columns in unemployment\nprint(unemployment.describe())\n\n             2010        2011        2012        2013        2014        2015  \\\ncount  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000   \nmean     8.409286    8.315440    8.317967    8.344780    8.179670    8.058901   \nstd      6.248887    6.266795    6.367270    6.416041    6.284241    6.161170   \nmin      0.450000    0.320000    0.480000    0.250000    0.200000    0.170000   \n25%      4.015000    3.775000    3.742500    3.692500    3.625000    3.662500   \n50%      6.965000    6.805000    6.690000    6.395000    6.450000    6.170000   \n75%     10.957500   11.045000   11.285000   11.310000   10.695000   10.215000   \nmax     32.020000   31.380000   31.020000   29.000000   28.030000   27.690000   \n\n             2016        2017        2018        2019        2020        2021  \ncount  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000  \nmean     7.925879    7.668626    7.426429    7.243736    8.420934    8.390879  \nstd      6.045439    5.902152    5.818915    5.696573    6.040915    6.067192  \nmin      0.150000    0.140000    0.110000    0.100000    0.210000    0.260000  \n25%      3.800000    3.690000    3.625000    3.487500    4.285000    4.335000  \n50%      5.925000    5.650000    5.375000    5.240000    6.695000    6.425000  \n75%     10.245000   10.315000    9.257500    9.445000   11.155000   10.840000  \nmax     26.540000   27.040000   26.910000   28.470000   29.220000   33.560000  \n\n\nAhora haz aprendido que unemployment contiene 182 filas de datos de países, incluyendo country_code, country_name, continent y porcentajes de desempleo desde 2010 hasta 2021. ¡Si miraste muy de cerca, podrías haber notado que a algunos países les falta información en la columna continent! Continuemos explorando estos datos en el próximo ejercicio.\n\n\n\nContar valores categóricos\nRecordemos del ejercicio anterior que el DataFrame unemployment contiene 182 filas de datos de países que incluyen country_code, country_name, continent y porcentajes de desempleo de 2010 a 2021.\nAhora vas a explorar los datos categóricos contenidos en unemployment para comprender los datos que contiene relacionados con cada continente.\n\nInstrucciones\n\nUtiliza un método para contar los valores asociados a cada continent en el DataFrame unemployment.\n\n\n# Count the values associated with each continent in unemployment\nprint(unemployment['continent'].value_counts())\n\ncontinent\nAfrica           53\nAsia             47\nEurope           39\nNorth America    18\nSouth America    12\nOceania           8\nName: count, dtype: int64\n\n\n¿Sabías que hay 23 países en América del Norte, que incluye países en el Caribe y América Central? Puede que hayas notado que América del Norte tiene 18 puntos de datos en el DataFrame unemployment, por lo que nos falta información de algunos de los países en nuestro conjunto de datos.\n\n\n\nDesempleo mundial en 2021\n¡Es hora de explorar algunos de los datos numéricos en unemployment! ¿Cuál fue el desempleo típico en un año determinado? ¿Cuál era la tasa de desempleo mínima y máxima, y cómo era la distribución de las tasas de desempleo en el mundo? Un histogrpama es una buena forma de hacerse una idea de las respuestas a estas preguntas.\nTu tarea en este ejercicio es crear un histograma que muestre la distribución de las tasas de paro mundiales en 2021.\n\nInstrucciones\n\nImporta las bibliotecas de visualización necesarias\nCrea un histograma de la distribución de los porcentajes de desempleo de 2021 en todos los países en unemployment; muestra un punto pocentual completo en cada casilla.\n\n\n# Import the required visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of 2021 unemployment; show a full percent in each bin\nsns.histplot(x='2021', data=unemployment, binwidth=1)\nplt.show()\n\n\n\n\n\n\n\n\nParece que el desempleo en el 2021 se mantuvo alrededor del 3% al 8% para la mayoría de los países en el conjunto de datos, pero algunos países experimentaron un desempleo muy alto del 20% al 35%.",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#validación-de-datos",
    "href": "01_Conocer_un_conjunto_de_datos.html#validación-de-datos",
    "title": "Conocer un conjunto de datos",
    "section": "Validación de datos",
    "text": "Validación de datos\n\nValidando los tipos de datos\n\n\nbooks.dtypes\n\n\n\nActualizando los tipos de datos\n\n\nbooks['year'] = books['year'].astype(int)\nbooks.dtypes\n\n\n\n\n\nTipo\nNombre Python\n\n\n\n\nString\nstr\n\n\nInteger\nint\n\n\nFloat\nfloat\n\n\nDictionary\ndict\n\n\nList\nlist\n\n\nBoolean\nbool\n\n\n\n\nValidando datos categóricos\n\n\nbooks['genre'].isin(['Fiction', 'Non Fiction'])\n\n\nPara validar los datos que no están en la lista\n\n~books['genre'].isin(['Fiction', 'Non Fiction'])\n\n\nPara filtrar el DataFrame por los valores en nuestra lista\n\nbooks[books['genre'].isin(['Fiction', 'Non Fiction'])].head()\n\n\n\nValidando los datos numéricos\n\nPara ver solo las columnas numéricas en un DataFrame:\n\nbooks.select_dtypes('number').head()\n\nPara conocer un intervalo específico:\n\nbooks['year'].min()\n\n\n\nbooks['year'].max()\n\n\nSe puede ver una imagen más detallada de la distribución de los datos, utilizando boxplot:\n\nsns.boxplot(data=books, x='year')\nplt.show()\n\n\nTambién se puede ver los datos agrupados por una variable categórica.\n\nsns.boxplot(data=books, x='year', y='genre')\nplt.show()\n\n\n\nDetectar tipos de datos\n¡Se ha modificado una columna en el DataFrame unemployment y ahora tiene un tipo de datos incorrecto! Este tipo de datos te impedirá realizar una exploración y un análisis eficaces, por lo que tu tarea consiste en identificar qué columna tiene un tipo de datos incorrecto y, a continuación, corregirlo.\n\nInstrucciones\nPregunta\n\n¿Cuál de las siguientes columnas requiere una actualización de su tipo de datos?\n\n\nprint(unemployment.dtypes)\n\ncountry_code     object\ncountry_name     object\ncontinent        object\n2010            float64\n2011            float64\n2012            float64\n2013            float64\n2014            float64\n2015            float64\n2016            float64\n2017            float64\n2018            float64\n2019             object\n2020            float64\n2021            float64\ndtype: object\n\n\nRespuestas posibles\n\ncountry_name\ncontinent\n2019\n2021\n\n\n\n\n\nActualiza el tipo de datos de la columna 2019 de unemployment a float.\n¡Vuelve a imprimir el dtypes del DataFrame umemployment para comprobar que se ha actualizado el tipo de datos!\n\n\n# Update the data type of the 2019 column to a float\nunemployment['2019'] = unemployment['2019'].astype('float')\n\n# Print the dtypes to check your work\nprint(unemployment.dtypes)\n\ncountry_code     object\ncountry_name     object\ncontinent        object\n2010            float64\n2011            float64\n2012            float64\n2013            float64\n2014            float64\n2015            float64\n2016            float64\n2017            float64\n2018            float64\n2019            float64\n2020            float64\n2021            float64\ndtype: object\n\n\nCambiar el tipo de dato de la columna 2019 significa que ahora puedes realizar cálculos sobre ella, incluyendo validar su rango.\n\n\n\nValidar continentes\nTu colega te ha informado de que los datos sobre el desempleo de los países de Oceanía no son fiables, y te gustaría identificar y excluir a estos países de tus datos de unemployment. ¡La función .isin() puede ayudarte con eso!\nTu tarea consiste en utilizar isin() para identificar los países que no están en Oceanía. Estos países deberían devolver True mientras que los países de Oceanía deberán devolver False. Esto te permitirá utilizar los resultados de isin() para filtrar rápidamente los países de Oceanía utilizando la indexación booleana.\n\n\nInstrucciones\n\nDefina una Serie Booleana que describan si cada continent está o no fuera de Oceanía; llama a esta Serie not_oceania.\n\n\n# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment['continent'].isin(['Oceania'])\n\n\nUtiliza la indexación booleana para imprimir el DataFrame unemployment sin ninguno de los datos relacionados con los países de Oceanía.\n\n\n# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment['continent'].isin(['Oceania'])\n\n# Print unemployment without records related  to countries in Oceania\nprint(unemployment[not_oceania])\n\n    country_code          country_name      continent   2010   2011   2012  \\\n0            AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1            AGO                Angola         Africa   9.43   7.36   7.35   \n2            ALB               Albania         Europe  14.09  13.48  13.38   \n3            ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4            ARG             Argentina  South America   7.71   7.18   7.22   \n..           ...                   ...            ...    ...    ...    ...   \n175          VNM               Vietnam           Asia   1.11   1.00   1.03   \n178          YEM           Yemen, Rep.           Asia  12.83  13.23  13.17   \n179          ZAF          South Africa         Africa  24.68  24.64  24.73   \n180          ZMB                Zambia         Africa  13.19  10.55   7.85   \n181          ZWE              Zimbabwe         Africa   5.21   5.37   5.15   \n\n      2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0    11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1     7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2    15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3     2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4     7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n175   1.32   1.26   1.85   1.85   1.87   1.16   2.04   2.39   2.17  \n178  13.27  13.47  13.77  13.43  13.30  13.15  13.06  13.39  13.57  \n179  24.56  24.89  25.15  26.54  27.04  26.91  28.47  29.22  33.56  \n180   8.61   9.36  10.13  10.87  11.63  12.01  12.52  12.85  13.03  \n181   4.98   4.77   4.78   4.79   4.78   4.80   4.83   5.35   5.17  \n\n[174 rows x 15 columns]\n\n\nValidaste datos categóricos y usaste tu validación .isin() para excluir datos en los que no estabas interesado. Filtrar los datos que no necesitas al comienzo de tu proceso de EDA es una excelente manera de organizarte para la exploración que está por venir.\n\n\nRango de validación\nAhora es el momento de validar nuestros datos numéricos. En la lección anterior vimos, utilizando .describe(), que la mayor tasa de desempleo durante 2021 fue de casi el 34 %, mientras que la más baja estuvo justo por encima de cero.\nTu tarea en este ejercicio es obtener información mucho más detallada sobre el rango de los datos de unemployment utilizando el diagrama de caja de Seaborn, y también visualizarás el rango de las tasas de desempleo en cada continente para comprender las diferencias de rango geográfico.\n\nInstrucciones\n\nImprime las tasas de desempleo mínima y máximam en este orden, durante 2021.\nCrea un diagrama de caja de las tasas de desempleo de 2021 (en el eje x), desglosadas por continente (en el eje y).\n\n\n# Print the minimum an maximum unemployment rates during 2021\nprint(unemployment['2021'].min(), unemployment['2021'].max())\n\n# Create a boxplot of 2021 unemployment rates, broken down by continent\nsns.boxplot(data=unemployment, x='2021', y='continent', \n            hue='continent', legend=False)\nplt.show()\n\n0.26 33.56\n\n\n\n\n\n\n\n\n\nObserva cómo varían los rangos de desempleo entre continentes. Por ejemplo, el percentil 50 de África es más bajo que el de América del Norte, pero el rango es mucho más amplio.",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#resumen-de-datos",
    "href": "01_Conocer_un_conjunto_de_datos.html#resumen-de-datos",
    "title": "Conocer un conjunto de datos",
    "section": "Resumen de datos",
    "text": "Resumen de datos\n\nExplorando grupo de datos\n\n.groupby() grupo de datos por categoría.\nFunción de agregación indica cómo se resume un grupo de datos.\n\n\n\nbooks.groupby('genre').mean()\n\n\n\nFunciones de agregación\n\nSuma: .sum()\nConteo: .cont\nMínimo: .min()\nMáximo: .max()\nVarianza: .var()\nDesviación estándar: .std()\n\nAgregación de datos no agrupados\n\n.agg() aplica funciones de agregación a través de un DataFrame\nPor defecto agrega todas las filas de una columna determinada\nSe suele utilizar cuando queremos más de una función\nSolo lo aplica a las columnas numéricas\n\n\n\nbooks.agg(['mean', 'std'])\n\n\n\nEspecificando agregaciones para columnas\n\n\nbooks.agg({'rating': ['mean', 'std'], 'year': ['median']})\n\n\n\nNombrando columnas resumen\n\n\nbooks.groupby('genre').agg(\n    mean_rating = ('rating', 'mean'),\n    std_rating = ('rating', 'std'),\n    median_year = ('year', 'median')\n)\n\n\n\nVisualizando resúmenes categoricos\n\nCalculan automáticamente la media de una variable cuantitativa\n\n\n\nsns.barplot(data=books, x='genre', y='rating')\nplt.show()\n\n\n\nResúmenes con .groupby() y .agg()\nEn este ejercicio, explorarás las medias y desviaciones estándar de los datos anuales de desempleo. En primer lugar, encontrarás las medias y desviaciones estándar independientemente del continente para observar las tendencias mundiales del desempleo. Después, comprobarás las tendencias del desempleo desglosadas por continente.\n\nimport pandas as pd\n\nruta = './data/clean_unemployment.csv'\nunemployment = pd.read_csv(ruta)\nprint(unemployment.head())\n\n  country_code          country_name      continent   2010   2011   2012  \\\n0          AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1          AGO                Angola         Africa   9.43   7.36   7.35   \n2          ALB               Albania         Europe  14.09  13.48  13.38   \n3          ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4          ARG             Argentina  South America   7.71   7.18   7.22   \n\n    2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0  11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1   7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2  15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3   2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4   7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n\n\n\nInstrucciones\n\nImprime la media y las desviación estándar d elas tasas de paro de cada año (en ese orden).\n\n\n# Print the mean and standard deviation of rates by year\nprint(unemployment[\n    ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n].agg(['mean', 'std']))\n\n          2010      2011      2012      2013      2014      2015      2016  \\\nmean  8.409286  8.315440  8.317967  8.344780  8.179670  8.058901  7.925879   \nstd   6.248887  6.266795  6.367270  6.416041  6.284241  6.161170  6.045439   \n\n          2017      2018      2019      2020  \nmean  7.668626  7.426429  7.243736  8.420934  \nstd   5.902152  5.818915  5.696573  6.040915  \n\n\n\nImprime la media y la desviación estándar (en ese orden) de las tasas de paro de cada año agrupadas por continente.\n\n\n# Print yearly mean and standard deviation grouped by continent\nprint(unemployment[\n    ['continent', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n].groupby(\"continent\").agg(['mean', 'std']))\n\n                    2010                 2011                 2012            \\\n                    mean       std       mean       std       mean       std   \ncontinent                                                                      \nAfrica          9.343585  7.411259   9.369245  7.401556   9.240755  7.264542   \nAsia            6.240638  5.146175   5.942128  4.779575   5.835319  4.756904   \nEurope         11.008205  6.392063  10.947949  6.539538  11.325641  7.003527   \nNorth America   8.663333  5.115805   8.563333  5.377041   8.448889  5.495819   \nOceania         3.622500  2.054721   3.647500  2.008466   4.103750  2.723118   \nSouth America   6.870833  2.807058   6.518333  2.801577   6.410833  2.936508   \n\n                    2013                 2014            ...      2016  \\\n                    mean       std       mean       std  ...      mean   \ncontinent                                                ...             \nAfrica          9.132453  7.309285   9.121321  7.291359  ...  9.277547   \nAsia            5.852128  4.668405   5.853191  4.681301  ...  6.094894   \nEurope         11.466667  6.969209  10.971282  6.759765  ...  9.394615   \nNorth America   8.840556  6.081829   8.512222  5.801927  ...  7.941111   \nOceania         3.980000  2.640119   3.976250  2.659205  ...  3.877500   \nSouth America   6.335000  2.808780   6.347500  2.834332  ...  7.230833   \n\n                             2017                2018                2019  \\\n                    std      mean       std      mean       std      mean   \ncontinent                                                                   \nAfrica         7.459439  9.284528  7.407620  9.237925  7.358425  9.264340   \nAsia           5.051796  6.171277  5.277201  6.090213  5.409128  5.949149   \nEurope         5.822793  8.359744  5.177845  7.427436  4.738206  6.764359   \nNorth America  5.503090  7.391111  5.326446  7.281111  5.253180  7.095000   \nOceania        2.477866  3.872500  2.492834  3.851250  2.455893  3.773750   \nSouth America  3.052309  7.281667  3.398994  7.496667  3.408856  7.719167   \n\n                              2020            \n                    std       mean       std  \ncontinent                                     \nAfrica         7.455293  10.307736  7.928166  \nAsia           5.254008   7.012340  5.699609  \nEurope         4.124734   7.470513  4.071218  \nNorth America  4.770490   9.297778  4.963045  \nOceania        2.369068   4.273750  2.617490  \nSouth America  3.379845  10.275000  3.411263  \n\n[6 rows x 22 columns]\n\n\nEstos datos están bien resumidos, pero es un poco largo. ¿Qué pasaríasi quisieras enfocarte en un resumen de solo un año y hacerlo más legible? ¡Inténtalo en el siguiente ejercicio!\n\n\n\nAgregaciones con nombre\nYa has visto cómo .groupby() y .agg() pueden combinarse para mostrar resúmenes para categorías. A veces, es útil nombrar nuevas columnas al agregar, para que se quede claro en la salida del código qué agregaciones se están aplicando y dónde.\nTu tarea consiste en crear un DataFrame llamado continent_summary que muestre una fila por cada continente. Las columnas del DataFram,e contendrán la tasa de paro media de cada continente en 2021, así como la desviación estándar de la tasa de empleo del 2021. Y por supuesto, ¡renombrarás las columnas para que su contenido quede claro!\n\nInstrucciones\n\nCrea una columna llamada mean_rate_2021 que muestre la tasa de paro media de 2021 para cada continente.\nCrea una columna llamada std_rate_2021 que muestre la desviación estándar de la tasa de paro de 2021 para cada continente.\n\n\ncontinent_sumary = unemployment[\n    ['continent', '2021']\n].groupby('continent').agg(\n    # Create the mean_rate_2021 column\n    mean_rate_2021 = ('2021', 'mean'),\n    # Create the std_rate_2021 column\n    std_rate_2021 = ('2021', 'std'),\n)\nprint(continent_sumary)\n\n               mean_rate_2021  std_rate_2021\ncontinent                                   \nAfrica              10.473585       8.131636\nAsia                 6.906170       5.414745\nEurope               7.414872       3.947825\nNorth America        9.155000       5.076482\nOceania              4.280000       2.671522\nSouth America        9.924167       3.611624\n\n\nEl desempleo promedio de 2021 varió ampliamente por continente, y también lo hizo el desempleo dentro de esos continentes.\n\n\n\nVisualizar resúmenes categóricos\nComo has aprendido en este capítulo, Seaborn tiene muchas visualizaciones estupendas para la exploración, incluido un gráfico de barras para mostrar un valor medio agregado por categoría de datos.\nEn Seaborn, los gráficos de barras incluyen una barra vertical que indica el intervalo de confianza del 95 % para la media categórica. Como los intervalos de confianza se calculan utilizando tanto el número de valores como la variabilidad de esos valores, dan una indicación útil de hasta qué punto se puede confiar en los datos.\nTu tarea consiste en crear un diagrama de barras para visualizar las medias y los intervalos de confianza de las tasas de desempleo en los distintos continentes.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nInstrucciones\n\nCrea un diagrama de barras que muestre los continentes en el eje x y lsus respectivas tasas medias de desempleo en 2021 en el eje y.\n\n\n# Create a bar plot of continents and their average unemployment\nsns.barplot(data=unemployment, x='continent', y='2021',\n            hue='continent', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\nAunque Europa tiene un mayor desempleo promedio que Asia, también tiene un intervalo de confianza más pequeño para ese promedio, por lo que el valor promedio es más confiable.",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "index.html#descripción",
    "href": "index.html#descripción",
    "title": "Análisis Exploratorio de Datos en Python",
    "section": "Descripción",
    "text": "Descripción\nAsí que tienes algunos datos interesantes, ¿por dónde empiezas tu análisis? Este curso cubrirá el proceso de exploración y análisis de datos, desde la comprensión de lo que se incluye en un conjunto de datos hasta la incorporación de los resultados de la exploración a un flujo de trabajo de ciencia de datos.\nUtilizando datos sobre cifras de desempleo y precios de billetes de avión, aprovecharás Python para resumir y validar datos, calcular, identificar y reemplazar valores perdidos, y limpiar valores numéricos y categóricos. A lo largo del curso, crearás hermosas visualizaciones Seaborn para comprender las variables y sus relaciones.\nPor ejemplo, examinarás cómo se relacionan el consumo de alcohol y el rendimiento de los alumnos. Por último, el curso mostrará cómo los hallazgos exploratorios alimentan los flujos de trabajo de la ciencia de datos creando nuevas características, equilibrando características categóricas y generando hipótesis a partir de los hallazgos.\nAl final de este curso, tendrás la confianza necesaria para realizar tu propio análisis exploratorio de datos (EDA) en Python. ¡Serás capaz de explicar tus conclusiones visualmente a los demás y sugerir los siguientes pasos para recopilar información a partir de tus datos!",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html",
    "href": "02_Limpieza_e_imputacion_de_datos.html",
    "title": "Limpieza e imputación de datos",
    "section": "",
    "text": "Tratar los datos que faltan\nExplorar y analizar datos a menudo significa tratar con valores perdidos, tipos de datos incorrectos y valores atípicos. En este capítulo, aprenderás técnicas para gestionar estos problemas y agilizar tus procesos en EDA.\nprint(salaries.isna().sum())\nthreshold = len(salaries) * 0.05\nprint(threhold)\ncols_to_drop = salaries.columns[salaries.isna().sum() &lt;= threshold]\nprint(cols_to_drop)\nsalaries.dropna(subset=cols_to_drop, inplace=True) # Para actualizar el DataFrame\ncols_with_missing_values = salaries.columns[salaries.isna().sum() &gt; 0]\nprint(cols_with_missing_values)\nfor col in cols_with_missing_values[:-1]:\n        salaries[col].fillna(salaries[col].mode()[0])\nprint(salaries.isna().sum())\nsalaries_dict = salaries.groupby('Experience')['Salary_USD'].median().to_dict()\nprint(salaries_dict)\nsalaries['Salary_USD'] = salaries['Salary_USD'].fillna(salaries['Experience'].map(salaries_dict))",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Limpieza e imputación de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#convertir-y-analizar-datos-categóricos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#convertir-y-analizar-datos-categóricos",
    "title": "Limpieza e imputación de datos",
    "section": "Convertir y analizar datos categóricos",
    "text": "Convertir y analizar datos categóricos\n\nPrevisualizar los datos\n\n\nprint(salaries.select_dtypes('object').head())\n\n\n\nTítulos de los trabajos\n\n\nprint(salaries['Designation'].value_counts())\n\n\n\nprint(salaries['Designation'].nunique())\n\n\n\n\nExtrayendo valores desde las categorías\n\nEl formato actual de los datos limita la capacidad de generar información.\npandas.Series.str.contains()\n\nBusca en una columna una cadena especifica o múltiples cadenas.\n\n\n\n\nsalaries['Designation'].str.contains('Scientist')\n\n\n\nFiltrar filas que contienen una o más frases\n\nPalabras de interes: Machine Learning o AI\n\n\n\nsalaries['Designation'].str.contains('Machine Learning|AI')\n\n\n\nBuscar múltiples frases en una cadena de caracteres\n\nPalabras de interes: Cualquiera que inicie con Data\n\n\n\nsalaries['Designation'].str.contains('ˆData')\n\n\nAhora que se tiene una idea de cómo funciona este método, definamos una lista de títulos de trabajo que queremos encontrar:\n\njob_categories = ['Data Science', 'Data Analytics',\n                   'Data Engineering', 'Machine Learning',\n                   'Managerial', 'Consultant']\n\nLuego necesitamos crear variables que contengan nuestros filtros\n\ndata_science = 'Data Scientist|NLP'\ndata_analyst = 'Analyst|Analytics'\ndata_engineer = 'Data Engineer|ETL|Architect|Infrastructure'\nml_engineer = 'Machine Learning|ML|Bid Data|AI'\nmanager = 'Manager|Head|Director|Lead|Principal|Staff'\nconsultant = 'Consultant|Freelance'\n\nEl siguiente paso es crear una lista con nuestro rango de condiciones para el método str.contains\n\nconditions = [\n    (salaries['Designation'].str.contains(data_science)),\n    (salaries['Designation'].str.contains(data_analyst)),\n    (salaries['Designation'].str.contains(data_engineer)),\n    (salaries['Designation'].str.contains(ml_engineer)),\n    (salaries['Designation'].str.contains(manager)),\n    (salaries['Designation'].str.contains(consultant))\n]\n\nFinalmente, podemos crear nuestra nueva columna Job_Category usando la función de selección de Numpy\n\nsalaries['Job_Category'] = np.select(conditions,\n                                     job_categories,\n                                     default='Other')\n\nAl obtener una vista previa de la Designación y nuestra nueva columna Job_Category, podemos verificar los primeros cinco valores.\n\nprint(salaries[['Designation', 'Job_Category']].head())\n\n\n\nVisualización de la frecuencia de la categoría job\n\n\nsns.countplot(data=salaries, x='Job_Category')\nplt.show()\n\n\n\nEncontrar el número de valores únicos\nTe gustaría practicar algunas de las habilidades de manipulación y análisis de datos categóricos que acabas de ver. Para ayudarte a identificar qué datos podrían reformatearse para extraer valor, vas a averiguar qué columnas no numéricas del conjunto de datos planes tienen un gran número de valores únicos.\n\nInstrucciones\n\nFiltra planes para las columnas que sean del tipo datos \"object\".\nRecorre las columnas del conjunto de datos.\nAñade el iterador de columna a la sentencia print y, a continucación, llama a la función para que devuelva el número de valores únicos de la columna.\n\n\n# Filter the DataFrame for objects columns\nnon_numeric = planes.select_dtypes('object')\n\n# Loop through columns\nfor column in non_numeric.columns:\n    # Print the number of unique values\n    print(f\"Number of unique values in {column} column: {non_numeric[column].nunique()}\")\n\nNumber of unique values in Airline column: 8\nNumber of unique values in Date_of_Journey column: 44\nNumber of unique values in Source column: 5\nNumber of unique values in Destination column: 6\nNumber of unique values in Route column: 122\nNumber of unique values in Dep_Time column: 218\nNumber of unique values in Arrival_Time column: 1220\nNumber of unique values in Duration column: 362\nNumber of unique values in Total_Stops column: 5\n\n\nCuriosamente, \"Duration\" es actualmente una columna de tipo objeto cuando debería ser una columna numérica, ¡y tiene 362 valores únicos! Vamos a averiguar más sobre esta columna.\n\n\n\nCategoría de duración de vuelos\nComo has visto, hay 362 valores únicos en la columna \"Duration\" de planes. Llamando a planes['Duration'].head(), vemos los siguientes valores.\n\n\n0        19h\n1     5h 25m\n2     4h 45m\n3     2h 25m\n4    15h 30m\nName: Duration, dtype: object\n\n\nParece que no será sencillo convertirlo a números. Sin embargo, ¡podrías clasificar los vuelos por duración y examinar la frecuencia de las distintas longitudes de vuelo!\nCrearás una columna \"Duration_Category\" en el DataFrame planes. Antes tendrás que crear una lista de valores que deseas insertar en el DataFrame, seguida de los valores existentes a partir de los cuales deben crearse.\n\nInstrucciones\n\nCrea una lista de categorías que contengan \"Short-haul\", \"Medium\" y \"Long-haul\".\n\n\n# Create a list of categories\nflight_categories = ['Short-haul', 'Medium', 'Long-haul']\n\n\n\n\n\nCrea short_flights, una cadena para capturar valores de \"0h\", \"1h\", \"2h\", \"3h\", \"4h\" teniendo cuidado de evitar valores como \"10h\".\nCrea medium_flights para capturar cualquier valor entre cinco y nueve horas. ~\nCrea long_flights para capturar cualquier valor comprendido entre 10 y 16 horas, ambos inclusive.\n\n\n# Create a list of categories\nflight_categories = ['Short-haul', 'Medium', 'Long-haul']\n\n# Create short-haul values\nshort_flights = '^0h|^1h|^2h|^3h|^4h'\n\n# Create medium-haul values\nmedium_flights = '^5h|^6h|^7h|^8h|^9h'\n\n# Create long-haul values\nlong_flights = '^10h|^11h|^12h|^13h|^14h|^15h|^16h'\n\nAhora has creado tus categorías y valores, es hora de agregar condicionalmente las categorías en el DataFrame\n\n\n\nAñadir categorías de duración\nAhora que has configurado las categorías y los valores que quieres capturar, ¡es hora de construir una nueva columna para analizar la frecuencia de los vuelos según su duración!\nLas variablesflight_categories, short_flights, medium_flights y long_flights que creaste anteriormente están a tu disposición.\n\nimport numpy as np\n\n\nInstrucciones\n\nCrea conditions, una lista que contenga subconjuntos de planes['Duration'] basados en short_flights, medium_flights y long_flights.\nCrea la columna \"Duration_Category\" llamando a una función que acepte tu lista conditions y flight_categories, estableciendo los valores no encontrados en \"Extreme duration\".\nCrea un gráfico fque muestre el recuento de cada categoría.\n\n\n# Create conditions for values in flight_categories to be created\nconditions = [\n    (planes['Duration'].str.contains(short_flights)),\n    (planes['Duration'].str.contains(medium_flights)),\n    (planes['Duration'].str.contains(long_flights))\n]\n\n# Apply the conditions list to the flight_categories\nplanes['Duration_Category'] = np.select(conditions, flight_categories,\n                                        default='Extreme duration')\n\n# Plot the counts of each categoryß\nsns.countplot(data=planes, x='Duration_Category',\n              hue='Duration_Category', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\n¡Está claro que la mayoría de los vuelos son de corta distancia.",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Limpieza e imputación de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#trabajar-con-datos-numéricos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#trabajar-con-datos-numéricos",
    "title": "Limpieza e imputación de datos",
    "section": "Trabajar con datos numéricos",
    "text": "Trabajar con datos numéricos\n\nEl dataset origina de los salarios\n\n\nprint(salaries.info())\n\n\n\nSalario en Rupias\n\n\nprint(salaries['Salary_In_Rupees'].head())\n\n\n\nConvirtiendo cadena de caracteres en números\n\nRemover las comas de los valores Salary_In_Rupees\nConvertir la columna a tipo de dato float\nCrear una nueva columna convirtiendo la moneda a dólares\n\n\n\npd.Series.str.replace('Caracter a remover', 'Caracter a reemplazar')\n\n\nsalaries['Salary_In_Rupees'] = salaries['Salary_In_Rupees'].str.replace(',', '')\nprint(salaries['Salary_In_Rupees'].head())\n\n\n\nsalaries['Salary_In_Rupees'] = salaries['Salary_In_Rupees'].astype(float) \n\n1 Indian Rupee = 0.012 US Dollars\n\nsalaries['Salary_USD'] = salaries['Salary_In_Rupees'] * 0.012\n\n\nPrevisualizando la nueva columna\n\n\nprint(salaries[['Salary_In_Rupees', 'Salary_USD']].head())\n\n\n\nAñadiendo un resumen esrtadístico al DataFrame\n\n\nsalaries.groupby('Company_Size')['Salary_USD'].mean()\n\n\nCalculo de la desviación estándar de los salarios por experiencia:\n\n\nsalaries['std_dev'] = salaries.groupby('Experience') \\ \n                      ['Salary_USD'].transform(lambda x: x.std())\n\n\nprint(salaries[['Experience', 'std_dev']].value_counts())\n\n\nRepitamos el proceso para otros datos estadísticos:\n\nsalaries['median_by_comp_size'] = salaries.groupby('Company_Size') \\\n                                  ['Salary_USD'].transform(lambda x: x.median())\n\n\nprint(salaries[['Company_Size', 'median_by_comp_size']].head())\n\n\n\nDuración del vuelo\nTe gustaría analizar la duración de los vuelos, pero por desgracia, la columna \"Duration\" de DataFrame planes contiene actualmente valores de cadena.\nTendrás que limpiar la columna y convertirla al tipo de datos correcto para el análisis.\n\nimport re\n\ndef duration_to_decimal_str(duration_str: str) -&gt; str:\n    '''\n    Convierte una duración de vuelo de formato '2h 30m' a una cadena en formato decimal en horas, como '2.5h'.\n    \n    Parámetros:\n    -----------\n    duration_str : str\n        Cadena de texto que representa la duración de un vuelo, como '2h 30m', '45m', '19h', etc.\n\n    Retorna:\n    --------\n    str\n        Cadena con duración expresada en horas decimales, con un solo decimal y el sufijo 'h'. Ej: '2.5h'\n    '''\n    horas = re.search(r'(\\d+)\\s*h', duration_str)\n    minutos = re.search(r'(\\d+)\\s*m', duration_str)\n\n    h = int(horas.group(1)) if horas else 0\n    m = int(minutos.group(1)) if minutos else 0\n\n    decimal_hours = round(h + m / 60, 1)\n    return f'{decimal_hours}h'\n\n\n\nplanes['Duration'] = planes['Duration'].apply(duration_to_decimal_str)\n\n\nInstrucciones\n\nImprime los cinco primeros valores de la columna \"Duration\".\n\n\n# Preview the column\nprint(planes['Duration'].head())\n\n0    19.0h\n1     5.4h\n2     4.8h\n3     2.4h\n4    15.5h\nName: Duration, dtype: object\n\n\n\nRetira \"h\" de la columna\n\n\n# Remove the string character\nplanes['Duration'] = planes['Duration'].str.replace('h', '')\nprint(planes['Duration'].head())\n\n0    19.0\n1     5.4\n2     4.8\n3     2.4\n4    15.5\nName: Duration, dtype: object\n\n\n\nConvierte la columna al tipo de datos float.\n\n\n# Convert to float data type\nplanes['Duration'] = planes['Duration'].astype(float)\nprint(planes['Duration'].head())\n\n0    19.0\n1     5.4\n2     4.8\n3     2.4\n4    15.5\nName: Duration, dtype: float64\n\n\n\nTraza un histograma de los valores de \"Duration\"\n\n\n# Plot a histogram\nsns.histplot(data=planes, x='Duration')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAñadir estadísticas descriptivas\nAhora \"Duration\" y \"Price\"contienen valores numéricos en el DataFrame planes, y te gustaría calcular para ellos estadísticas de resumen condicionadas a los valores de otras columnas.\n\nInstrucciones\n\nAñade una columna a planes que contenga la desviación estándar de \"Price\" basada en \"Airline\".\n\n\n# Price standard deviation by Airline\nplanes['airline_price_st_dev'] = planes.groupby('Airline')['Price'].transform(lambda x: x.std())\nprint(planes[['Airline', 'airline_price_st_dev']].value_counts())\n\nAirline            airline_price_st_dev\nJet Airways        4159.846432             3082\nIndiGo             2245.529140             1632\nAir India          3692.609285             1399\nMultiple carriers  3558.323763              959\nSpiceJet           1798.900648              653\nVistara            2888.915498              376\nAir Asia           1979.826234              260\nGoAir              2764.926625              147\nName: count, dtype: int64\n\n\n\nCalcula la mediana de \"Duration\" en \"Airline\", almacenándola como una columna llamada \"airline_median_duration\".\n\n\n# Median Duration by Airline\nplanes['airline_median_duration'] = planes.groupby('Airline')['Duration'].transform(lambda x: x.median())\nprint(planes[['Airline', 'airline_median_duration']].value_counts())\n\nAirline            airline_median_duration\nJet Airways        13.3                       3082\nIndiGo             2.9                        1632\nAir India          15.5                       1399\nMultiple carriers  10.2                        959\nSpiceJet           2.5                         653\nVistara            3.2                         376\nAir Asia           2.8                         260\nGoAir              2.9                         147\nName: count, dtype: int64\n\n\n\nEncuenta la media \"Price\" por \"Destination\", guardándola como una columna llamada \"price_destination_mean\".\n\n\n# Mean Price by Destination\nplanes['price_destination_mean'] = planes.groupby('Destination')['Price'].transform(lambda x: x.mean())\nprint(planes[['Destination', 'price_destination_mean']].value_counts())\n\nDestination  price_destination_mean\nCochin       10473.585927              3631\nBanglore     9093.622872               2291\nDelhi        5248.541082                998\nNew Delhi    11579.306944               720\nHyderabad    5190.274021                562\nKolkata      4907.156863                306\nName: count, dtype: int64\n\n\nParece que Jet Airways tiene la mayor desviación estándar en precio, Air India tiene la mayor duración median, y Nueva Delhi, en promedio, es el destiono más caro. Ahora veamos cómo manejar los datos atípicos.",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Limpieza e imputación de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#gestión-de-valores-atípicos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#gestión-de-valores-atípicos",
    "title": "Limpieza e imputación de datos",
    "section": "Gestión de valores atípicos",
    "text": "Gestión de valores atípicos\n\nQué es un outlier?\n\nEs una observación que está muy alejada de otros puntos de datos.\n\nUsando estadística descriptiva\n\n\nprint(salaries['Salary_USD'].describe())\n\n\n\nUsando el rango intercuartil\n\nRango intercuartil (IQR)\n\nIQR = 75th - 25th percentil\nUpper outliers &gt; 75th percentile + (1.5 * IQR)\nLower Outliers &lt; 25th percentile - (1.5 * IQR)\n\n\n\n\nsns.boxplot(data=salaries, y='Salaary_USD')\nplt.show()\n\n\n\nIdentificando Umbrales\n\n\n# 75th percentil\nseventy_fifth = salaries['Salary_USD'].quantile(0.75)\n\n# 25th percentil\ntwenty_fifth = salaries['Salary_USD'].quantile(0.25)\n\n# Interquartil range\nsalaries_iqr = seventy_fifth - twenty_fifth\n\nprint(salaries_iqr)\n\n\n\n# Upper threshold\nupper = seventy_fifth + (1.5 * salaries_iqr)\n\n# Lower threshold\nlower = twenty_fifth - (1.5 * salaries_iqr)\n\nprint(upper, lower)\n\n\n\nSubdividiendo nuestros datos\n\n\nsalaries[(salaries['Salary_USD'] &lt; lower) | (salaries['Salary_USD'] &gt; upper)] \\\n        [['Experience', 'Employee_Location', 'Salary_USD']]\n\n\n\n¿ Por qué buscar los Outliers?\n\nLos Outliers son valores extremos\n\nPueden no representar con precisión los datos\n\nPueden sesgar la media y la desviación estándar\nPruebas de estadística y modelos de machine learning requieren datos que tengan una distribución normal y no esten sesgados.\n\nQué hacer con los Outliers?\n\nPreguntas que nos debemos hacer:\n\nPor qué existen los outliers?\nLos valores son precisos?\n\n\nEliminación de Outliers\n\n\nno_outliers = salaries[(salaries['Salar_USD'] &gt; lower) & (salaries['Salary_USD'] &lt; upper)]\n\n\nprint(no_outliers['Salary_USD'].describe())\n\n\n\nDistribución de Salarios\n\n\n\nQué hacer con los valores atípicos?\nIdentificar los valores atípicos es un paso integral en la realización de análisis exploratorios de datos.\nEn este ejercicio, se te presentarán escenarios en los que hay valores atípicos, y tendrás que decidir qué acción debes tomar.\n\nInstrucciones\n\nColoca cada escenario en el cubo adecuado en función del enfoque que deba adaptarse para tratar los valores atípicos.\n\n\n\n\n\n\n\nElimina los valores atípicos\nDejar los valores atípicos en el conjunto de datos\n\n\n\n\nUn sensor de temperatura tiene un registro de 100 grados Celsius, pero el sensor solo funciona correctamente a temperaturas de hasta 80 grados.\nse registran las alturas de distintos animales y uno de ellos es más de 1.5 veces la IQR más el percentil 75.\n\n\nLa velocidad de un coche se registra como 5000 km/h.\nLos países tienen una superficie total media de 667.143 km2, pero un país tiene 1.637.687 km2.\n\n\nUn participante en un estudio tiene una edad de menos 35 años.\nUn jugador de baloncesto hace una media de 35 puntos por partido cuando la media en toda la liga es de solo 10 puntos por partido.\n\n\n\n\nPuede ser difícil decidir qué hacer con los valores atípicos, pero debes saber cómo gestionarlos, ¡ya que a menudo se dan en el mundo real!\n\n\n\nIdentificar valores atípicos\nHas demostrado que reconoces qué hacer cuando se te presentan valores atípicos, pero ¿Puedes identificarlos utilizando visualizaciones?\nIntenta averiguar si hay valores atípicos en las columnas \"Price\" o \"Duration\" del dataframe planes.\n\nIntrucciones\n\nTraza la distribución de la columna \"Price\" de planes.\n\n\n# Plot a histogram of flight prices\nsns.histplot(data=planes, x='Price')\nplt.show()\n\n\n\n\n\n\n\n\n\nMuestra las estadísticas descriptivas de la duración del vuelo.\n\n\n# Display descriptive statistics for flight duration\nprint(planes['Duration'].describe())\n\ncount    8508.000000\nmean       10.726704\nstd         8.472415\nmin         0.100000\n25%         2.800000\n50%         8.700000\n75%        15.500000\nmax        47.700000\nName: Duration, dtype: float64\n\n\n\nPregunta\n\n¿Qué columna contiene potencialmente valores atípicos?\nRespuestas Posibles\n\n\"Price\"\n\"Duration\"\n\"Price\" y \"Duration\"\nNinguna\n\nLos histogramas, diagramas de caja y estadísticas descriptivas también son métodos útiles para identificar valores extremos. ¡Ahora vamos a tratarlos!\n\n\n\nEliminar valores atípicos\nAunque eliminar los valores atípicos no siempre es el camino a seguir, para tu análisis has decidido que solo incluirás los vuelos en los que el \"Price\" no sea un valor atípico.\nPor lo tanto tienes que encontrar el umbral superior y utilizarlo para eliminar los valores que lo superen del Dataframe planes.\n\nInstrucciones\n\nHalla los percentiles 75 y 25, guardando como price_seventy_fifth y price_twenty_fifth respectivamente.\n\n\n# Find the 75th and 25th percentiles\nprice_seventy_fifth = planes['Price'].quantile(0.75)\nprice_twenty_fifth = planes['Price'].quantile(0.25)\n\n\nCalcula el IQR, almacenándolo como prices_iqr.\n\n\n# Calculate iqr\nprices_iqr = price_seventy_fifth - price_twenty_fifth\nprint(prices_iqr)\n\n7014.0\n\n\n\nCalcula los umbrales superior e inferior de los valores atípicos.\n\n\n# Calculate the thresholds\nupper = price_seventy_fifth + (1.5 * prices_iqr)\nlower = price_twenty_fifth - (1.5 * prices_iqr)\n\n\nElimina los valores atípicos de planes.\n\n\n# Subset the data\nplanes = planes[(planes['Price'] &gt; lower) & (planes['Price'] &lt; upper)]\n\nprint(planes['Price'].describe())\n\ncount     8438.000000\nmean      8877.466046\nstd       4001.838236\nmin       1759.000000\n25%       5224.000000\n50%       8372.000000\n75%      12121.000000\nmax      22270.000000\nName: Price, dtype: float64\n\n\n¡Habilidades ridículas para eliminar valores atípicos! Lograste crear umbrales basados en el IQR y los usaste para filtrar el conjunto de datos planes para eliminar precios extremos. Originalmente, el conjunto de datos tenía un precio máximo de casi 55000, pero la salida de planes['Price'].describe() muestra que el máximo se ha reducido a alrededor de 23000, ¡reflejando una distribución menos sesgada para el análisis!",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Limpieza e imputación de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#tratar-los-datos-que-faltan",
    "href": "02_Limpieza_e_imputacion_de_datos.html#tratar-los-datos-que-faltan",
    "title": "Limpieza e imputación de datos",
    "section": "",
    "text": "Por qué un dato faltante es un problema?\n\nAfectan las distribuciones\nLos datos de la población son menos repreesntativos\nPuede resultar en conclusiones incorrectas\n\nEjemplo datos de profesionales de datos\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nData type\n\n\n\n\nWorking_Year\nYear the data was obtained\nFloat\n\n\nDesignation\nJob title\nString\n\n\nExperience\nExperience level e.g., \"Mid\", \"Senior\"\nString\n\n\nEmployment_Satus\nType of employment contract e.g., \"FT\", \"PT\"\nString\n\n\nEmployee_Location\nCountry of employment\nString\n\n\nCompany_Size\nLabels for company size e.g., \"S\", \"M\", \"L\"\nString\n\n\nRemote_Working_Ratio\nPorcentage of time working remotely\nInteger\n\n\nSalary_USD\nSalary in US dollars\nFloat\n\n\n\n\nRevisando los datos faltantes\n\n\n\n\nEstrategias para el manejo de datos faltantes\n\nEliminar los datos faltantes\n\n5 % o menos del total de valores\n\nImputar la media, mediana o la moda\n\nDepende de la distribución y contexto\n\nImputar por sub-grupos\n\nDiferentes niveles de experiencia tienen diferente mediana en el salario\n\n\nEliminando valores faltantes\n\n\n\n\n\n\n\nImputando una estadística de resumen\n\n\n\n\n\nRevisando los valores faltantes que faltan\n\n\n\n\nImputando por subgrupo\n\n\n\n\n\n\nTratar los datos que faltan\nEs importante tratar los datos que faltan antes de empezar el análisis.\nUn enfoque consiste en descartar los valores que faltan si representan una pequeña proporción, normalmente el 5 %, de los datos.\nTrabajando con un conjunto de datos sobre precios de tiquetes de avión, almacenado como un DataFrame de pandas llamado planes, tendrás que contar el número de valores perdidos en todas las columnas, calcular el cinco porciento de todos los valores, utilizar este umbral para eliminar observaciones y comprobar cuántos valores perdidos quedan en el conjunto de datos.\n\nimport pandas as pd\n\nruta = './data/planes.csv'\nplanes = pd.read_csv(ruta)\nprint(planes.head)\n\n&lt;bound method NDFrame.head of            Airline Date_of_Journey    Source Destination  \\\n0      Jet Airways       9/06/2019     Delhi      Cochin   \n1           IndiGo      12/05/2019   Kolkata    Banglore   \n2           IndiGo      01/03/2019  Banglore   New Delhi   \n3         SpiceJet      24/06/2019   Kolkata    Banglore   \n4      Jet Airways      12/03/2019  Banglore   New Delhi   \n...            ...             ...       ...         ...   \n10655     Air Asia       9/04/2019   Kolkata    Banglore   \n10656    Air India      27/04/2019   Kolkata    Banglore   \n10657  Jet Airways      27/04/2019  Banglore       Delhi   \n10658      Vistara      01/03/2019  Banglore   New Delhi   \n10659    Air India       9/05/2019     Delhi      Cochin   \n\n                       Route Dep_Time  Arrival_Time Duration Total_Stops  \\\n0      DEL → LKO → BOM → COK    09:25  04:25 10 Jun      19h     2 stops   \n1            CCU → NAG → BLR    18:05         23:30   5h 25m      1 stop   \n2            BLR → NAG → DEL    16:50         21:35   4h 45m      1 stop   \n3                  CCU → BLR    09:00         11:25   2h 25m    non-stop   \n4            BLR → BOM → DEL    18:55  10:25 13 Mar  15h 30m      1 stop   \n...                      ...      ...           ...      ...         ...   \n10655              CCU → BLR    19:55         22:25   2h 30m    non-stop   \n10656              CCU → BLR    20:45         23:20   2h 35m    non-stop   \n10657              BLR → DEL      NaN         11:20       3h    non-stop   \n10658              BLR → DEL    11:30         14:10   2h 40m    non-stop   \n10659  DEL → GOI → BOM → COK    10:55         19:15   8h 20m     2 stops   \n\n                   Additional_Info    Price  \n0                          No info  13882.0  \n1                          No info   6218.0  \n2                          No info  13302.0  \n3                          No info   3873.0  \n4      In-flight meal not included  11087.0  \n...                            ...      ...  \n10655                      No info   4107.0  \n10656                      No info   4145.0  \n10657                          NaN   7229.0  \n10658                      No info  12648.0  \n10659                      No info  11753.0  \n\n[10660 rows x 11 columns]&gt;\n\n\n\nInstrucciones\n\nImprime el número de valores perdidos en cada columna del DataFrame\n\n\n# Count the number of missing values in each column\nprint(planes.isna().sum())\n\nAirline            427\nDate_of_Journey    322\nSource             187\nDestination        347\nRoute              256\nDep_Time           260\nArrival_Time       194\nDuration           214\nTotal_Stops        212\nAdditional_Info    589\nPrice              616\ndtype: int64\n\n\n\nCalcula a cuántas observaciones equivale el cinco porciento del DataFrame planes\n\n\n# Find the five percent threshold\nthreshold = len(planes) * 0.05\nprint(threshold)\n\n533.0\n\n\n\n\n\n\nCrea cols_to_drop aplicando una indexación booleana a las columnas del DataFrame con valores perdidos menores o iguales que el umbral.\nUtiliza este filtro para eliminar los valores que faltan y guardar el DataFrame actualizado.\n\n\n# Create a filter\ncols_to_drop = planes.columns[planes.isna().sum() &lt;= threshold]\nprint(cols_to_drop)\n\n# Drop missing values for columns below the threshold\nplanes.dropna(subset=cols_to_drop, inplace=True)\n\nprint(planes.isna().sum())\n\nIndex(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops'],\n      dtype='object')\nAirline              0\nDate_of_Journey      0\nSource               0\nDestination          0\nRoute                0\nDep_Time             0\nArrival_Time         0\nDuration             0\nTotal_Stops          0\nAdditional_Info    300\nPrice              368\ndtype: int64\n\n\nAl crear un umbral de valores faltantes y usarlo para filtrar columnas, haz logrado eliminar los valores faltantes de todas las columnas excepto \"Additinal_Info\" y \"Price\".\n\n\n\nEstrategias para datos que faltan\nLa regla del cinco porciento ha funcionado muy bien en tu conjunto de dato planes, ¡eliminando los valores perdidos de nueve de las 11 columnas!\nAhora tienes que decidir qué hacer con las columnas \"Additional_Info\" y \"Price\", a las que les faltan los valores 300 y 368 respectivamente.\nPrimero echarás un vistazo a lo que contiene \"Additional_Info\", y después visualizarás el precio de los billetes de avión de distintas compañías aéreas.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nInstrucciones\n\nImprime los valores y frecuencias de \"Additional_Info\".\n\n\n# Check the values of the Additional_Info column\nprint(planes['Additional_Info'].value_counts())\n\nAdditional_Info\nNo info                         6399\nIn-flight meal not included     1525\nNo check-in baggage included     258\n1 Long layover                    14\nChange airports                    7\nNo Info                            2\nBusiness class                     1\nRed-eye flight                     1\n2 Long layover                     1\nName: count, dtype: int64\n\n\n\nCrea un boxplot de \"Price\" frente a \"Airline\"\n\n\n# Create a box plot of Price by Airline\nsns.boxplot(data=planes, x='Airline', y='Price',\n            hue='Airline', legend=False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPregunta\n\n\n¿Cómo debes tratar los valores que faltan en \"Additional_Info\" y \"Price\".?\n\nRespuestas Posibles\n\nElimina la columna \"Additional_Info\" e imputa la media para los valores que faltan de \"Price\".\nElimina los valores de \"No info\" de \"Additiona_Info\" e imputa la mediana de los valores que faltan de \"Price\".\nElimina la columna \"Additional_Info\" e imputa la media por \"Airline\" para los valores que falten de \"Price\".\nElimina la columna \"Additional_Info\" e imputa la mediana por \"Airline\" para los valores que falten de \"Price\".\n\nNo necesitamos la columna \"Additional_Info\", y deberías imputar la mediana de \"Price\" por \"Airline\" para representar los datos con precisión.\n\n\n\nImputar los precios de los aviones que faltan\n!Ahora solo queda una columna con valores perdidos!\nHas eliminado la columna \"Additional_Info\" de planes, el último paso es imputar los datos que faltan en la columna \"Price\" del conjunto de datos.\nComo recordatorio, tú generaste este diagrama de caja, que sugería que imputar el precio medio basándose en el \"Airline\" ¡es un enfoque sólido!\n\n# Eliminamos la columna Additional_Info\nplanes = planes.drop('Additional_Info', axis=1)\nplanes.columns\n\nIndex(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops', 'Price'],\n      dtype='object')\n\n\n\nInstrucciones\n\nAgrupa planes por aerolínea y calcula el precio medio.\n\n\n# Calculate median plane ticket prices by Airplane\nairline_prices = planes.groupby('Airline')['Price'].median()\n\nprint(airline_prices)\n\nAirline\nAir Asia              5192.0\nAir India             9443.0\nGoAir                 5003.5\nIndiGo                5054.0\nJet Airways          11507.0\nMultiple carriers    10197.0\nSpiceJet              3873.0\nVistara               8028.0\nName: Price, dtype: float64\n\n\n\nConvierte los precios medios agrupados en un diccionario.\n\n\n# Convert to a dictionary\nprices_dict = airline_prices.to_dict()\nprint(prices_dict)\n\n{'Air Asia': 5192.0, 'Air India': 9443.0, 'GoAir': 5003.5, 'IndiGo': 5054.0, 'Jet Airways': 11507.0, 'Multiple carriers': 10197.0, 'SpiceJet': 3873.0, 'Vistara': 8028.0}\n\n\n\n\n\n\nImputa condicionalmente los valores perdidos de \"Price\" asignando los valores de la columna \"Airline\" en función de prices_dict\nComprueba si faltan valores\n\n\n# Map the dictionary to missing values of Price by Airline\nplanes['Price'] = planes['Price'].fillna(planes['Airline'].map(prices_dict))\n\n# Check for missing values\nprint(planes.isna().sum())\n\nAirline            0\nDate_of_Journey    0\nSource             0\nDestination        0\nRoute              0\nDep_Time           0\nArrival_Time       0\nDuration           0\nTotal_Stops        0\nPrice              0\ndtype: int64\n\n\nConvertiste un DataFrame agrupado a un diccionario y luego lo usaste para llenar condicionalmente los valores faltantes de \"Price\" basándote en \"Airline\". Ahora vamos a explorar cómo realizar análisis exploratorio en datos categóricos.",
    "crumbs": [
      "Capítulos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Limpieza e imputación de datos</span>"
    ]
  }
]